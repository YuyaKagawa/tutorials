{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html  \n",
    "Author: Adam Paszke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このチュートリアルでは、PyTorchを使ってDeepQLearningをします。OpenAI GymのCartPole-v0タスクを解きます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# タスク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エージェントは2つのアクションのうちから決定する必要があります。カートを左に動かすか、右に動かすかです。  \n",
    "このとき、上に立てたポールが倒れないようにします。  \n",
    "https://gym.openai.com/envs/CartPole-v0 でリーダーボードを確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./cartpole.gif\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エージェントが現在の環境を観測してアクションを選択したら、環境は新しい状態に遷移し、アクションの結果に対する報酬を返します。  \n",
    "このタスクでは、各タイムステップで報酬が+1ずつ増加していきます。  \n",
    "そしてポールが倒れるかカートが中心から2.4よりも離れた場合に環境が終了します。  \n",
    "つまりより長くキープできることがより良いということになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPoleのタスクでは、エージェントへの入力は環境の状態を表す4つの値（位置、速度など）です。  \n",
    "毎フレームで計算します。  \n",
    "状態は前のフレームと現在のフレームの差とします。こうするとポールの速度も扱えます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パッケージのインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最初に必要なパッケージをインポートします。環境のためにgymを入れます。他にも以下のものを使います。  \n",
    "<ul>\n",
    "    <li>torch.nn: ニューラルネットワーク</li>\n",
    "    <li>torch.optim: 最適化</li>\n",
    "    <li>torch.autograd: 自動微分</li>\n",
    "    <li>torchvision: 画像関連で有用</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/anaconda3/lib/python3.7/site-packages\")\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"CartPole-v0\").unwrapped # 環境\n",
    "\n",
    "### matplotlibのセットアップ\n",
    "is_ipython=\"inline\" in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    \n",
    "plt.ion()\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # デバイス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQNの学習において、experience replay memoryを使います。  \n",
    "これはエージェントが観察してきた遷移を保存します。後々このデータを再利用します。  \n",
    "これからランダムにサンプリングすることで、バッチを形成した遷移が無相関となります（？）。  \n",
    "これによってDQNの学習が安定化することが知られています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを実現するために、以下の2つのクラスを用意します。  \n",
    "<ul>\n",
    "    <li>Transition: 環境における、一つの遷移を表す。(state,action)→(next_state,reward)のマッピング。</li>\n",
    "    <li>ReplayMemory: 最近の遷移を持つcyclic buffer。sample()で遷移のバッチからランダムに抽出できる。</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition=namedtuple(\"Transition\",(\"state\",\"action\",\"next_state\",\"reward\"))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.capacity=capacity\n",
    "        self.memory=[]\n",
    "        self.position=0\n",
    "        \n",
    "    def push(self,*args): # 遷移を保存する\n",
    "        if len(self.memory)<self.capacity:\n",
    "            self.memory.append(None)\n",
    "            \n",
    "        self.memory[self.position]=Transition(*args)\n",
    "        self.position=(self.position+1)%self.capacity\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではこれからモデルを定義します。がその前に、DQNを理解しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_main)",
   "language": "python",
   "name": "conda_main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
