{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考:  \n",
    "https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/  \n",
    "https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/LSTM_starter.ipynb  \n",
    "https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMの基礎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/LSTM_starter.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずはLSTMの基礎的な部分について見てみましょう。  \n",
    "PyTorchのLSTMのレイヤがどのように動いているのか、出力を可視化することで見てみましょう。  \n",
    "モデルのレイヤの動きを見るためにインスタンス化する必要はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:17:11.520482Z",
     "start_time": "2020-05-01T08:17:11.514759Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMのパラメータとしていろいろなものが設定できますが、ここでは入力の次元と隠れ層の次元と層の数を指定します。  \n",
    "\n",
    "<ul>\n",
    "    <li>Input dimension: 各タイムステップでの入力のサイズを示します。 e.g. サイズが5の場合、[1,3,8,2,3]</li>\n",
    "    <li>Hidden dimension: 各タイムステップでの隠れ層の状態とセル状態のサイズを示します。 e.g. 次元が3の場合、共に[3,5,4]</li>\n",
    "    <li>Number of layers: LSTMの層の数</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:17:12.375835Z",
     "start_time": "2020-05-01T08:17:12.372587Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim=5 # 入力の次元\n",
    "hidden_dim=10 # 隠れ層の次元\n",
    "n_layers=1 # 層の数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力の動きを見るために、試しにデータを生成してみます。入力の次元を5とすると、求められるテンソルの形は(1,1,5)です。（（バッチサイズ、シーケンスの長さ、入力の次元））  \n",
    "加えて、LSTMを最初のセルとしているので、隠れ層の状態とセル状態を初期化する必要があります。これらはそれぞれ（隠れ層の状態、セル状態）の形式で格納されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:17:14.276205Z",
     "start_time": "2020-05-01T08:17:14.265513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 1, 5])\n",
      "Hidden shape: (torch.Size([1, 1, 10]),torch.Size([1, 1, 10]))\n"
     ]
    }
   ],
   "source": [
    "batch_size=1 # バッチサイズ\n",
    "seq_len=1 # シーケンスの長さ\n",
    "\n",
    "inp=torch.randn(batch_size,seq_len,input_dim) # 入力データはランダムに\n",
    "\n",
    "### レイヤの用意\n",
    "lstm_layer=nn.LSTM(input_dim,hidden_dim,n_layers,batch_first=True) # LSTMレイヤ\n",
    "hidden_state=torch.randn(n_layers,batch_size,hidden_dim) # 内部状態\n",
    "cell_state=torch.randn(n_layers,batch_size,hidden_dim) # セル\n",
    "hidden=(hidden_state,cell_state) # タプルで\n",
    "\n",
    "print(\"Input shape: {}\".format(inp.shape))\n",
    "print(\"Hidden shape: ({},{})\".format(hidden[0].shape,hidden[1].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上で用意したLSTMのレイヤにデータを入力してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:17:16.159989Z",
     "start_time": "2020-05-01T08:17:16.152087Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([1, 1, 10])\n",
      "Hidden:  (tensor([[[-0.0449, -0.3824, -0.1499, -0.4087,  0.1130, -0.1120,  0.7433,\n",
      "           0.0934,  0.3570,  0.3538]]], grad_fn=<StackBackward>), tensor([[[-0.0628, -0.8317, -0.7305, -0.9544,  0.3643, -0.2340,  1.4044,\n",
      "           0.2279,  0.8152,  0.6376]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "out,hidden=lstm_layer(inp,hidden) # LSTMの出力\n",
    "print(\"Output shape: \",out.shape)\n",
    "print(\"Hidden: \",hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上のプロセスでは、各ステップでLSTMのセルがどのように入力と隠れ層の状態を扱っているのかを見てきました。  \n",
    "しかし多くのケースでは、大きい文章で入力データを扱います。  \n",
    "LSTMは様々な長さのシーケンスを入力とし、各タイムステップで出力することができます。それではシーケンスの長さを変えてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:17:17.226692Z",
     "start_time": "2020-05-01T08:17:17.218917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_len=3 # シーケンスの長さ\n",
    "inp=torch.randn(batch_size,seq_len,input_dim) # 入力データ\n",
    "out,hidden=lstm_layer(inp,hidden) # LSTMの出力\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、出力の第2次元は3です。これはLSTMの出力は3つであることを意味します。  \n",
    "入力のシーケンスの長さと一致します。  \n",
    "文章生成のように各タイムステップで出力が必要な場合、各タイムステップでの出力は第2次元から直接取得され、全結合層に入力されます。  \n",
    "感情分析などの文章分類のタスクでは、最後のタイムステップでの出力が分類器の入力となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:17:19.149049Z",
     "start_time": "2020-05-01T08:17:19.144091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "out=out.squeeze()[-1,:] # データの形を変換\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./LSTM_Cell_Equations.png\" width=\"250\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 感情分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考: https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KaggleのAmazonのカスタマーレビューのデータセットを使って感情分析をしてみます。  \n",
    "このデータセットには4,000,000ものレビューが含まれており、各レビューはポジティブもしくはネガティブのラベルが付けられています。  \n",
    "（ちなみにこのチュートリアルはFloydHubを使って動かせます。）  \n",
    "\n",
    "このチュートリアルの最終的な目標は、LSTMでレビューの感情の分類をすることです。  \n",
    "そのために、まずデータの前処理、モデルの定義、そしてモデルの評価をします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの用意"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:19:22.572705Z",
     "start_time": "2020-05-01T08:19:22.569516Z"
    }
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文法の辞書を入手します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T04:36:45.299596Z",
     "start_time": "2020-05-01T04:36:42.450413Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kagawa/anaconda3/envs/MAIN/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download(\"punkt\") # 文法データのダウンロード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T05:37:43.093207Z",
     "start_time": "2020-04-28T05:37:41.900325Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget  \"https://www.kaggle.com/bittlingmayer/amazonreviews/download/train.ft.txt.bz2\" -o ./../data/amazon_reviews/train.ft.txt.bz2\n",
    "!wget \"https://www.kaggle.com/bittlingmayer/amazonreviews/download/test.ft.txt.bz2\" -o ./../data/amazon_reviews/test.ft.txt.bz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファイルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:21:51.775055Z",
     "start_time": "2020-05-01T08:19:45.754356Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_file=bz2.BZ2File(\"./../data/amazon_reviews/train.ft.txt.bz2\") # 訓練用データセット\n",
    "test_file=bz2.BZ2File(\"./../data/amazon_reviews/test.ft.txt.bz2\") # テスト用データセット\n",
    "\n",
    "### ファイルを読み込むために、今回は下の処理も行う\n",
    "train_file=train_file.readlines()\n",
    "test_file=test_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットのサイズでも見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:21:51.784739Z",
     "start_time": "2020-05-01T08:21:51.778054Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training reviews: 3600000\n",
      "Number of test reviews: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training reviews: \"+str(len(train_file))) # 訓練用データセットのサイズ\n",
    "print(\"Number of test reviews: \"+str(len(test_file))) # テスト用データセットのサイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なんと訓練用データセットは3,600,000、テスト用は400,000もの大きさです。  \n",
    "時間を節約するためにこの全てのデータを使うことは避けます。もし余裕があるようでしたらもっと使ってもいいと思います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:21:55.399911Z",
     "start_time": "2020-05-01T08:21:51.797307Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_train=800000 # 訓練に使うデータ数\n",
    "num_test=200000 # テストに使うデータ数\n",
    "\n",
    "train_file=[x.decode(\"utf-8\") for x in train_file[:num_train]] # 訓練用データセットの文字コードを変換\n",
    "test_file=[x.decode(\"utf-8\") for x in test_file[:num_test]] # テスト用データセットの文字コードを変換\n",
    "\n",
    "print(train_file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、文章整形を進めます。  \n",
    "まず、文章からラベルを抽出します。フォーマットは、\"__label__1/2 &lt; sentence &gt;\" です。  \n",
    "ラベルは、ポジティブが1でネガティブが0です。  \n",
    "ここでは、全てのURLを\"&lt; url &gt; \"という形にします。感情分析にこれらは関係ありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:27:12.087046Z",
     "start_time": "2020-05-01T08:26:48.874090Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels=[0 if x.split(\" \")[0]==\"__label__1\" else 1 for x in train_file] # 訓練用ラベル\n",
    "train_sentences=[x.split(\" \",1)[1][:-1].lower() for x in train_file] # 訓練用文章\n",
    "test_labels=[0 if x.split(\" \")[0]==\"__label__1\" else 1 for x in test_file] # テスト用ラベル\n",
    "test_sentences=[x.split(\" \",1)[1][:-1].lower() for x in test_file] # テスト用文章\n",
    "\n",
    "for i in range(len(train_sentences)): # 訓練用文章について\n",
    "    train_sentences[i]=re.sub(\"\\d\",\"0\",train_sentences[i]) # \\dを0に変換\n",
    "\n",
    "for i in range(len(test_sentences)): # テスト用文章について\n",
    "    test_sentences[i]=re.sub(\"\\d\",\"0\",test_sentences[i]) # \\dを0に変換\n",
    "\n",
    "### URLを<url>に修正する\n",
    "for i in range(len(train_sentences)): # 訓練用文章について\n",
    "    if \"www.\" in train_sentences[i] or \"http:\" in train_sentences[i] or \"https:\" in train_sentences[i] or \"com\" in train_sentences[i]:\n",
    "        train_sentences[i]=re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\",\"<url>\",train_sentences[i])\n",
    "        \n",
    "for i in range(len(test_sentences)): # テスト用文章について\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（多分）メモリ節約のために、もう必要のない変数を削除しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T08:27:18.819683Z",
     "start_time": "2020-05-01T08:27:18.655424Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_file,test_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## トークン化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にトークン化しましょう。  \n",
    "この処理は自然言語処理のタスクではよくやられるものです。  \n",
    "これは文章をワードや文法などの各トークンに分けるものです。  \n",
    "これを行う際には辞書が必要です。spaCyやscikit-learnなどがありますが、NLTKを使うと早いです。  \n",
    "そしてワードは、ワード→出現回数のマッピングを持つ辞書に入れられます。これが語彙となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:00:24.094895Z",
     "start_time": "2020-05-01T08:48:04.858845Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "0.25% done\n",
      "0.5% done\n",
      "0.75% done\n",
      "1.0% done\n",
      "1.25% done\n",
      "1.5% done\n",
      "1.75% done\n",
      "2.0% done\n",
      "2.25% done\n",
      "2.5% done\n",
      "2.75% done\n",
      "3.0% done\n",
      "3.25% done\n",
      "3.5% done\n",
      "3.75% done\n",
      "4.0% done\n",
      "4.25% done\n",
      "4.5% done\n",
      "4.75% done\n",
      "5.0% done\n",
      "5.25% done\n",
      "5.5% done\n",
      "5.75% done\n",
      "6.0% done\n",
      "6.25% done\n",
      "6.5% done\n",
      "6.75% done\n",
      "7.0% done\n",
      "7.25% done\n",
      "7.5% done\n",
      "7.75% done\n",
      "8.0% done\n",
      "8.25% done\n",
      "8.5% done\n",
      "8.75% done\n",
      "9.0% done\n",
      "9.25% done\n",
      "9.5% done\n",
      "9.75% done\n",
      "10.0% done\n",
      "10.25% done\n",
      "10.5% done\n",
      "10.75% done\n",
      "11.0% done\n",
      "11.25% done\n",
      "11.5% done\n",
      "11.75% done\n",
      "12.0% done\n",
      "12.25% done\n",
      "12.5% done\n",
      "12.75% done\n",
      "13.0% done\n",
      "13.25% done\n",
      "13.5% done\n",
      "13.75% done\n",
      "14.0% done\n",
      "14.25% done\n",
      "14.5% done\n",
      "14.75% done\n",
      "15.0% done\n",
      "15.25% done\n",
      "15.5% done\n",
      "15.75% done\n",
      "16.0% done\n",
      "16.25% done\n",
      "16.5% done\n",
      "16.75% done\n",
      "17.0% done\n",
      "17.25% done\n",
      "17.5% done\n",
      "17.75% done\n",
      "18.0% done\n",
      "18.25% done\n",
      "18.5% done\n",
      "18.75% done\n",
      "19.0% done\n",
      "19.25% done\n",
      "19.5% done\n",
      "19.75% done\n",
      "20.0% done\n",
      "20.25% done\n",
      "20.5% done\n",
      "20.75% done\n",
      "21.0% done\n",
      "21.25% done\n",
      "21.5% done\n",
      "21.75% done\n",
      "22.0% done\n",
      "22.25% done\n",
      "22.5% done\n",
      "22.75% done\n",
      "23.0% done\n",
      "23.25% done\n",
      "23.5% done\n",
      "23.75% done\n",
      "24.0% done\n",
      "24.25% done\n",
      "24.5% done\n",
      "24.75% done\n",
      "25.0% done\n",
      "25.25% done\n",
      "25.5% done\n",
      "25.75% done\n",
      "26.0% done\n",
      "26.25% done\n",
      "26.5% done\n",
      "26.75% done\n",
      "27.0% done\n",
      "27.25% done\n",
      "27.5% done\n",
      "27.75% done\n",
      "28.0% done\n",
      "28.25% done\n",
      "28.5% done\n",
      "28.75% done\n",
      "29.0% done\n",
      "29.25% done\n",
      "29.5% done\n",
      "29.75% done\n",
      "30.0% done\n",
      "30.25% done\n",
      "30.5% done\n",
      "30.75% done\n",
      "31.0% done\n",
      "31.25% done\n",
      "31.5% done\n",
      "31.75% done\n",
      "32.0% done\n",
      "32.25% done\n",
      "32.5% done\n",
      "32.75% done\n",
      "33.0% done\n",
      "33.25% done\n",
      "33.5% done\n",
      "33.75% done\n",
      "34.0% done\n",
      "34.25% done\n",
      "34.5% done\n",
      "34.75% done\n",
      "35.0% done\n",
      "35.25% done\n",
      "35.5% done\n",
      "35.75% done\n",
      "36.0% done\n",
      "36.25% done\n",
      "36.5% done\n",
      "36.75% done\n",
      "37.0% done\n",
      "37.25% done\n",
      "37.5% done\n",
      "37.75% done\n",
      "38.0% done\n",
      "38.25% done\n",
      "38.5% done\n",
      "38.75% done\n",
      "39.0% done\n",
      "39.25% done\n",
      "39.5% done\n",
      "39.75% done\n",
      "40.0% done\n",
      "40.25% done\n",
      "40.5% done\n",
      "40.75% done\n",
      "41.0% done\n",
      "41.25% done\n",
      "41.5% done\n",
      "41.75% done\n",
      "42.0% done\n",
      "42.25% done\n",
      "42.5% done\n",
      "42.75% done\n",
      "43.0% done\n",
      "43.25% done\n",
      "43.5% done\n",
      "43.75% done\n",
      "44.0% done\n",
      "44.25% done\n",
      "44.5% done\n",
      "44.75% done\n",
      "45.0% done\n",
      "45.25% done\n",
      "45.5% done\n",
      "45.75% done\n",
      "46.0% done\n",
      "46.25% done\n",
      "46.5% done\n",
      "46.75% done\n",
      "47.0% done\n",
      "47.25% done\n",
      "47.5% done\n",
      "47.75% done\n",
      "48.0% done\n",
      "48.25% done\n",
      "48.5% done\n",
      "48.75% done\n",
      "49.0% done\n",
      "49.25% done\n",
      "49.5% done\n",
      "49.75% done\n",
      "50.0% done\n",
      "50.25% done\n",
      "50.5% done\n",
      "50.75% done\n",
      "51.0% done\n",
      "51.25% done\n",
      "51.5% done\n",
      "51.75% done\n",
      "52.0% done\n",
      "52.25% done\n",
      "52.5% done\n",
      "52.75% done\n",
      "53.0% done\n",
      "53.25% done\n",
      "53.5% done\n",
      "53.75% done\n",
      "54.0% done\n",
      "54.25% done\n",
      "54.5% done\n",
      "54.75% done\n",
      "55.0% done\n",
      "55.25% done\n",
      "55.5% done\n",
      "55.75% done\n",
      "56.0% done\n",
      "56.25% done\n",
      "56.5% done\n",
      "56.75% done\n",
      "57.0% done\n",
      "57.25% done\n",
      "57.5% done\n",
      "57.75% done\n",
      "58.0% done\n",
      "58.25% done\n",
      "58.5% done\n",
      "58.75% done\n",
      "59.0% done\n",
      "59.25% done\n",
      "59.5% done\n",
      "59.75% done\n",
      "60.0% done\n",
      "60.25% done\n",
      "60.5% done\n",
      "60.75% done\n",
      "61.0% done\n",
      "61.25% done\n",
      "61.5% done\n",
      "61.75% done\n",
      "62.0% done\n",
      "62.25% done\n",
      "62.5% done\n",
      "62.75% done\n",
      "63.0% done\n",
      "63.25% done\n",
      "63.5% done\n",
      "63.75% done\n",
      "64.0% done\n",
      "64.25% done\n",
      "64.5% done\n",
      "64.75% done\n",
      "65.0% done\n",
      "65.25% done\n",
      "65.5% done\n",
      "65.75% done\n",
      "66.0% done\n",
      "66.25% done\n",
      "66.5% done\n",
      "66.75% done\n",
      "67.0% done\n",
      "67.25% done\n",
      "67.5% done\n",
      "67.75% done\n",
      "68.0% done\n",
      "68.25% done\n",
      "68.5% done\n",
      "68.75% done\n",
      "69.0% done\n",
      "69.25% done\n",
      "69.5% done\n",
      "69.75% done\n",
      "70.0% done\n",
      "70.25% done\n",
      "70.5% done\n",
      "70.75% done\n",
      "71.0% done\n",
      "71.25% done\n",
      "71.5% done\n",
      "71.75% done\n",
      "72.0% done\n",
      "72.25% done\n",
      "72.5% done\n",
      "72.75% done\n",
      "73.0% done\n",
      "73.25% done\n",
      "73.5% done\n",
      "73.75% done\n",
      "74.0% done\n",
      "74.25% done\n",
      "74.5% done\n",
      "74.75% done\n",
      "75.0% done\n",
      "75.25% done\n",
      "75.5% done\n",
      "75.75% done\n",
      "76.0% done\n",
      "76.25% done\n",
      "76.5% done\n",
      "76.75% done\n",
      "77.0% done\n",
      "77.25% done\n",
      "77.5% done\n",
      "77.75% done\n",
      "78.0% done\n",
      "78.25% done\n",
      "78.5% done\n",
      "78.75% done\n",
      "79.0% done\n",
      "79.25% done\n",
      "79.5% done\n",
      "79.75% done\n",
      "80.0% done\n",
      "80.25% done\n",
      "80.5% done\n",
      "80.75% done\n",
      "81.0% done\n",
      "81.25% done\n",
      "81.5% done\n",
      "81.75% done\n",
      "82.0% done\n",
      "82.25% done\n",
      "82.5% done\n",
      "82.75% done\n",
      "83.0% done\n",
      "83.25% done\n",
      "83.5% done\n",
      "83.75% done\n",
      "84.0% done\n",
      "84.25% done\n",
      "84.5% done\n",
      "84.75% done\n",
      "85.0% done\n",
      "85.25% done\n",
      "85.5% done\n",
      "85.75% done\n",
      "86.0% done\n",
      "86.25% done\n",
      "86.5% done\n",
      "86.75% done\n",
      "87.0% done\n",
      "87.25% done\n",
      "87.5% done\n",
      "87.75% done\n",
      "88.0% done\n",
      "88.25% done\n",
      "88.5% done\n",
      "88.75% done\n",
      "89.0% done\n",
      "89.25% done\n",
      "89.5% done\n",
      "89.75% done\n",
      "90.0% done\n",
      "90.25% done\n",
      "90.5% done\n",
      "90.75% done\n",
      "91.0% done\n",
      "91.25% done\n",
      "91.5% done\n",
      "91.75% done\n",
      "92.0% done\n",
      "92.25% done\n",
      "92.5% done\n",
      "92.75% done\n",
      "93.0% done\n",
      "93.25% done\n",
      "93.5% done\n",
      "93.75% done\n",
      "94.0% done\n",
      "94.25% done\n",
      "94.5% done\n",
      "94.75% done\n",
      "95.0% done\n",
      "95.25% done\n",
      "95.5% done\n",
      "95.75% done\n",
      "96.0% done\n",
      "96.25% done\n",
      "96.5% done\n",
      "96.75% done\n",
      "97.0% done\n",
      "97.25% done\n",
      "97.5% done\n",
      "97.75% done\n",
      "98.0% done\n",
      "98.25% done\n",
      "98.5% done\n",
      "98.75% done\n",
      "99.0% done\n",
      "99.25% done\n",
      "99.5% done\n",
      "99.75% done\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "words=Counter() # 訓練用の文章に出てくるワードについて、ワード→該当するワードの出現回数 のdict\n",
    "\n",
    "for i,sentence in enumerate(train_sentences):\n",
    "    # 文章はワード/トークンのリストとしてリストに追加されてゆく\n",
    "    train_sentences[i]=[]\n",
    "    \n",
    "    for word in nltk.word_tokenize(sentence): # ワードをトークン化する\n",
    "        words.update([word.lower()]) # 全てのワードを小文字に変換する\n",
    "        train_sentences[i].append(word) # リストに追加\n",
    "        \n",
    "    if i%40000==0: # 2,000個の文章ごとに\n",
    "        print(str((i*100)/num_train)+\"% done\")\n",
    "        \n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 語彙の整理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "タイポや存在しないワードを排除するために、語彙から1回しか出現しなかったワードを削除します。  \n",
    "unknownやpaddingに対処するために、それらを語彙に追加もします。  \n",
    "各ワードには数字のインデックスが割り当てられ、これがマッピングとなります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:02:23.751996Z",
     "start_time": "2020-05-01T09:02:23.316942Z"
    }
   },
   "outputs": [],
   "source": [
    "words={k:v for k,v in words.items() if v>1} # 出現回数が2回以上のワードに限定する\n",
    "words=sorted(words,key=words.get,reverse=True) # 出現回数の多い順に並び替える\n",
    "words=[\"_PAD\",\"_UNK\"]+words # paddingとunknownを辞書に追加する\n",
    "\n",
    "word2idx={o:i for i,o in enumerate(words)} # ワード→インデックス\n",
    "idx2word={i:o for i,o in enumerate(words)} # インデックス→ワード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらのマッピングを使い、文章中の各ワードを変換してゆきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:02:57.403071Z",
     "start_time": "2020-05-01T09:02:24.585032Z"
    }
   },
   "outputs": [],
   "source": [
    "for i,sentence in enumerate(train_sentences): # 訓練用の文章について\n",
    "    train_sentences[i]=[word2idx[word] if word in word2idx else 1 for word in sentence] # 文章中の各ワードを番号に変換\n",
    "    \n",
    "for i,sentence in enumerate(test_sentences): # テスト用の文章について\n",
    "    test_sentences[i]=[word2idx[word] if word in word2idx else 1 for word in sentence] # 文章中の各ワードを番号に変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を早めるために、短い文章を0埋めで長くしたり、長い文章を短くしたりします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:03:18.190988Z",
     "start_time": "2020-05-01T09:02:57.405217Z"
    }
   },
   "outputs": [],
   "source": [
    "# 短い文章は0埋めして、長い文章は短くする関数\n",
    "def pad_input(sentences,seq_len):\n",
    "    features=np.zeros((len(sentences),seq_len),dtype=int) # 特徴量の長さ分の0\n",
    "    \n",
    "    for ii,review in enumerate(sentences): # 各文章について\n",
    "        if len(review)!=0: # もし長さが1以上の場合\n",
    "            features[ii,-len(review):]=np.array(review)[:seq_len] # 短い分だけ前を0埋め&長い分だけ短くする\n",
    "            \n",
    "    return features\n",
    "\n",
    "seq_len=200 # 統一された文章の長さ\n",
    "train_sentences=pad_input(train_sentences,seq_len) # 訓練用のシーケンス\n",
    "test_sentences=pad_input(test_sentences,seq_len) # テスト用のシーケンス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例えば下のような形に変換されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:03:18.215836Z",
     "start_time": "2020-05-01T09:03:18.194247Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0  65588     91     16      3 101561     13     11\n",
      "    195    461     18    364     15      9   5743      3  89872     14\n",
      "     77    437     36     94      5     51   1667      9     91      8\n",
      "    144     83    617  18637      2    214    129     15      5     27\n",
      "    540      3    214  18192   2034     22     56     10     35     10\n",
      "      3    794      5     27    132    540      9     58      3     99\n",
      "    129     15      9   7407    264     48   4580  61923      6    430\n",
      "      7  17664   1082     23   8893   2715      6   3939  19348      2\n",
      "      9     51   4798    207     83   2396      8    317     15  17001]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットは既に訓練用とテスト用に分割されています。  \n",
    "検証用の分も確保します。  \n",
    "テスト用のものの半分を検証用とします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証用のデータを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:03:30.393778Z",
     "start_time": "2020-05-01T09:03:30.301841Z"
    }
   },
   "outputs": [],
   "source": [
    "train_labels=np.array(train_labels) # numpyの形に変換\n",
    "test_labels=np.array(test_labels) # numpyの形に変換\n",
    "\n",
    "split_frac=0.5 # テスト用データと検証用データの比率\n",
    "split_id=int(split_frac*len(test_sentences)) # どこで区切るかのインデックス\n",
    "val_sentences,test_sentences=test_sentences[:split_id],test_sentences[split_id:] # 文章を検証用とテスト用に分割\n",
    "val_labels,test_labels=test_labels[:split_id],test_labels[split_id:] # ラベルを検証用とテスト用に分割"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データローダの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではPyTorchの機能を使ってこのデータのローダを用意します。  \n",
    "まずはPyTorchの型に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:04:44.091763Z",
     "start_time": "2020-05-01T09:04:44.082726Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "train_data=TensorDataset(torch.from_numpy(train_sentences),torch.from_numpy(train_labels)) # 訓練用データ\n",
    "val_data=TensorDataset(torch.from_numpy(val_sentences),torch.from_numpy(val_labels)) # 検証用データ\n",
    "test_data=TensorDataset(torch.from_numpy(test_sentences),torch.from_numpy(test_labels)) # テスト用データ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にデータローダを作ります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:04:53.696415Z",
     "start_time": "2020-05-01T09:04:53.688619Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size=400 # バッチサイズ\n",
    "train_loader=DataLoader(train_data,shuffle=True,batch_size=batch_size) # 訓練用データのローダ\n",
    "val_loader=DataLoader(val_data,shuffle=True,batch_size=batch_size) # 検証用データのローダ\n",
    "test_loader=DataLoader(test_data,shuffle=True,batch_size=batch_size) # テスト用データのローダ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どのデバイスを利用するかを決定します。  \n",
    "GPUを使える場合はGPUを使える設定にします。  \n",
    "（FloydHubを使っている場合はGPUを利用できます。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:05:39.379339Z",
     "start_time": "2020-05-01T09:05:39.374193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "is_cuda=torch.cuda.is_available() # GPUが利用可能かどうか\n",
    "\n",
    "if is_cuda: # GPUを利用できる場合\n",
    "    device=torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else: # GPUを利用できない場合\n",
    "    device=torch.device(\"cpu\")\n",
    "    print(\"GPU is not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一つのバッチを読み込んでみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:06:26.463210Z",
     "start_time": "2020-05-01T09:06:26.359762Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 200]) torch.Size([400])\n"
     ]
    }
   ],
   "source": [
    "dataiter=iter(train_loader) # ローダはこういう使い方もできる\n",
    "sample_x,sample_y=dataiter.next() # 一つバッチを読み込む\n",
    "print(sample_x.shape,sample_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではネットワークを構築します。LSTMのレイヤを持ったものにします。下のものでは、最初にembeddingをします。  \n",
    "最後の層では全結合+シグモイドをすることで感情をポジティブ・ネガティブに分類します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:07:46.052712Z",
     "start_time": "2020-05-01T09:07:46.036727Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentNet(nn.Module): # 感情分類ネットワーク\n",
    "    def __init__(self,vocab_size,output_size,embedding_dim,hidden_dim,n_layers,drop_prob=0.5):\n",
    "        super(SentimentNet,self).__init__()\n",
    "        self.output_size=output_size # 出力のサイズ\n",
    "        self.n_layers=n_layers # LSTMの層の数\n",
    "        self.hidden_dim=hidden_dim # LSTMの隠れ層の次元\n",
    "        \n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim) # embedding\n",
    "        self.lstm=nn.LSTM(embedding_dim,hidden_dim,n_layers,dropout=drop_prob,batch_first=True) # LSTMレイヤ\n",
    "        self.dropout=nn.Dropout(0.2) # ドロップアウト\n",
    "        self.fc=nn.Linear(hidden_dim,output_size) # 全結合層\n",
    "        self.sigmoid=nn.Sigmoid() # シグモイド\n",
    "        \n",
    "    def forward(self,x,hidden):\n",
    "        batch_size=x.size(0) # バッチごとに入力を得るとする\n",
    "        x=x.long() # データの型を変更\n",
    "        embeds=self.embedding(x) # embeddingをする\n",
    "        lstm_out,hidden=self.lstm(embeds,hidden) # LSTMの出力を取得\n",
    "        lstm_out=lstm_out.contiguous().view(-1,self.hidden_dim) # データの形を変換\n",
    "        \n",
    "        out=self.dropout(lstm_out) # ドロップアウト\n",
    "        out=self.fc(out) # 全結合\n",
    "        out=self.sigmoid(out) # シグモイド\n",
    "        \n",
    "        out=out.view(batch_size,-1) # データの形を変換\n",
    "        out=out[:,-1]\n",
    "        \n",
    "        return out,hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size): # 隠れ層の初期化\n",
    "        weight=next(self.parameters()).data \n",
    "        hidden=(\n",
    "            weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().to(device),\n",
    "            weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().to(device)\n",
    "        )\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloveやFastTextなどのembeddingを使えばモデルの学習もうまくいくでしょう。  \n",
    "出力の次元は1です。今回は0/1だけが必要なためです。  \n",
    "それではネットワークを用意します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T09:10:00.083448Z",
     "start_time": "2020-05-01T09:09:59.391850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentNet(\n",
      "  (embedding): Embedding(217631, 400)\n",
      "  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size=len(word2idx)+1 # 語彙のサイズ\n",
    "output_size=1 # 出力のサイズ\n",
    "embedding_dim=400 # embeddingの次元\n",
    "hidden_dim=512 # 隠れ層のサイズ\n",
    "n_layers=2 # LSTMのレイヤの数\n",
    "\n",
    "model=SentimentNet(vocab_size,output_size,embedding_dim,hidden_dim,n_layers) # モデル\n",
    "model.to(device) # デバイスに送る\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではモデルを学習させます。  \n",
    "各決められたステップごとに、検証用データセットに対する予測の性能も見ておきます。  \n",
    "state_dictはPyTorchのmodelの重みです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T00:16:28.622894Z",
     "start_time": "2020-05-01T09:12:08.775208Z"
    }
   },
   "outputs": [],
   "source": [
    "lr=0.005 # 学習率\n",
    "criterion=nn.BCELoss() # ロス関数\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr) # 最適化関数\n",
    "\n",
    "epochs=2 # エポック数\n",
    "counter=0 # データを何個読み込んだか\n",
    "print_every=1000 # 何回ごとに表示するか\n",
    "clip=5\n",
    "valid_loss_min=np.Inf # 検証ロスの最小値\n",
    "\n",
    "model.train() # モデルを学習させることを宣言\n",
    "\n",
    "for i in range(epochs): # エポックごとに\n",
    "    h=model.init_hidden(batch_size) # 内部状態の初期化\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        counter+=1\n",
    "        h=tuple([e.data for e in h])\n",
    "        inputs,labels=inputs.to(device),labels.to(device) # 入力とラベルをデバイスに送る\n",
    "               \n",
    "        model.zero_grad() # モデルの勾配を初期化する\n",
    "        output,h=model(inputs,h) # モデルの出力を得る\n",
    "        \n",
    "        loss=criterion(output.squeeze(),labels.float()) # ロスを計算\n",
    "        loss.backward() # 逆伝搬\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),clip) # ?\n",
    "        optimizer.step() # 最適化する\n",
    "        \n",
    "        if counter%print_every==0: # 表示するとき\n",
    "            val_h=model.init_hidden(batch_size) # 検証用の重みを初期化\n",
    "            val_losses=[] # 検証ロスのリスト\n",
    "            model.eval() # これから検証することを宣言\n",
    "            \n",
    "            for inp,lab in val_loader: # 検証用データの各バッチについて\n",
    "                val_h=tuple([each.data for each in val_h])\n",
    "                inp,lab=inp.to(device),lab.to(device) # デバイスに送る\n",
    "                out,val_h=model(inp,val_h) # モデルの出力を得る\n",
    "                val_loss=criterion(out.squeeze(),lab.float()) # 検証ロス\n",
    "                val_losses.append(val_loss.item()) # 検証ロス\n",
    "                \n",
    "            model.train() # 検証が済んだので再び学習に戻る\n",
    "            \n",
    "            print(\n",
    "                \"Epoch: {}/{}...\".format(i+1,epochs),\n",
    "                \"Step: {}...\".format(counter),\n",
    "                \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                \"Val Loss: {:.6f}\".format(np.mean(val_losses))\n",
    "            )\n",
    "            \n",
    "            if np.mean(val_losses)<=valid_loss_min: # もし検証ロスが最小を更新したら\n",
    "                torch.save(model.state_dict(),\"./state_dict.pt\") # ファイルに保存\n",
    "                print(\"Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...\".format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min=np.mean(val_losses) # 最小値を更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 精度の評価"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではテストデータについて、予測性能の評価をします。  \n",
    "前回の学習では、最も検証データへのロスが小さいときのモデルのパラメータを保存しました。これを利用します。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-02T00:16:28.636496Z",
     "start_time": "2020-05-01T09:13:24.596Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./state_dict.pt\")) # モデルのパラメータをロードする\n",
    "\n",
    "test_losses=[] # テストロスのリスト\n",
    "num_correct=0 # 正しく分類できた数\n",
    "h=model.init_hidden(batch_size) # 隠れ層の初期化\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for inputs,labels in test_loader: # テストデータの各バッチについて\n",
    "    h=tuple([each.data for each in h])\n",
    "    inputs,labels=inputs.to(device),labels.to(device)\n",
    "    output,h=model(inputs,h) # モデルの出力\n",
    "    test_loss=criterion(output.squeeze(),labels.float()) # テストロスを算出\n",
    "    test_losses.append(test_loss.item()) # ロスのリストに追加\n",
    "    pred=torch.round(output.squeeze()) # 値を0/1にすることで予測クラスとする\n",
    "    correct_tensor=pred.eq(labels.float().view_as(pred))\n",
    "    correct=np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct+=np.sum(correct) # 正しく分類できた数\n",
    "    \n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc=num_correct/len(test_loader.dataset) # テストの予測精度\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMを使った予測の精度を見ることができました。  \n",
    "ハイパーパラメータのチューニングもせずにこんなものでした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
